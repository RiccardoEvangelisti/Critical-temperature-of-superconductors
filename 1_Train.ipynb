{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critical Temperature of Superconductors\n",
    "\n",
    "- In order to compare in detail the results of different hyperparameters configurations, it is developed a system based on GridSearchcv[*](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) and Pipeline[*](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) that execute a single configuration at each execution, and save it into a csv file. A different csv file is generated for each model.\n",
    "    - Another advantage of this system is that the program execution can be stopped at any time without losing the already trained configurations\n",
    "    - The only downside is that the execution is not parallel, but the dataset is relateively small, thus not much time for each configuration execution\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import os\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "from utils import Step, Pipe, Combination, extract_combinations, combination_already_tested, print_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "DATA_FOLDER = \"data/\"\n",
    "OUTPUT_FOLDER = \"outputs/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "OUTLIER_REMOVAL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of Properties+Formula df:  (17010, 169)\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(DATA_FOLDER + \"formula_train.csv\").drop(columns=[\"critical_temp\"]),\n",
    "        pd.read_csv(DATA_FOLDER + \"train.csv\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "print(\"Shapes of Properties+Formula df: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"material\" feature\n",
    "df = df.drop(columns=\"material\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13608, 167), (3402, 167), (13608, 1), (3402, 1))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "X_train = train.drop(columns=[\"critical_temp\"])\n",
    "y_train = train[[\"critical_temp\"]]\n",
    "\n",
    "X_test = test.drop(columns=[\"critical_temp\"])\n",
    "y_test = test[[\"critical_temp\"]]\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Remove Highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: remove, useless\n",
    "# if REMOVE_HIGH_CORR_FEATURES:\n",
    "#     corr_matrix = df.corr().abs()\n",
    "#     upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "#     cols_to_drop = [column for column in upper.columns if any(upper[column] >= CORR_THRESHOLD)]\n",
    "\n",
    "#     print(\"{} Cols Removed: {}\".format(len(cols_to_drop), cols_to_drop))\n",
    "#     X_train = X_train.drop(columns=cols_to_drop)\n",
    "#     X_test = X_test.drop(columns=cols_to_drop)\n",
    "#     print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesRemover:\n",
    "    \"\"\"\n",
    "    Class that provide the fit and transform methods, in order to be used as a \"transformer\" into the Pipeline class\n",
    "    \"\"\"\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        corr_matrix = df.corr().abs()\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        cols_to_drop = [column for column in upper.columns if any(upper[column] >= self.corr_threshold)]\n",
    "\n",
    "        # print(\"{} Cols Removed: {}\".format(len(cols_to_drop), cols_to_drop))\n",
    "        X = X.drop(columns=cols_to_drop)\n",
    "        return X\n",
    "\n",
    "    def set_params(self, corr_threshold):\n",
    "        self.corr_threshold = corr_threshold\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OUTLIER_REMOVAL:\n",
    "    columns = train.columns\n",
    "    outliers = pd.Series(index=train.index, dtype=bool)\n",
    "\n",
    "    clf = LocalOutlierFactor(n_jobs=-1)\n",
    "    # clf = IsolationForest(\n",
    "    #     max_samples=1.0,\n",
    "    #     contamination=0.001,\n",
    "    #     n_jobs=-1,\n",
    "    #     random_state=random_state,\n",
    "    # )\n",
    "    outliers = clf.fit_predict(train) == -1\n",
    "\n",
    "    print(\"Outliers removed: {}\".format(outliers.sum()))\n",
    "    train = train[~outliers]\n",
    "\n",
    "    X_train = train.drop(columns=[\"critical_temp\"])\n",
    "    y_train = train[[\"critical_temp\"]]\n",
    "    print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Remove, wrong bc not applicable\n",
    "# class OutliersRemover:\n",
    "#     def __init__(self) -> None:\n",
    "#         self.outliers_vector = None\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         self.outliers_vector = pd.Series(index=X.index, dtype=bool)\n",
    "\n",
    "#         clf = LocalOutlierFactor(n_jobs=-1)\n",
    "#         # clf = IsolationForest(\n",
    "#         #     max_samples=1.0,\n",
    "#         #     contamination=0.001,\n",
    "#         #     n_jobs=-1,\n",
    "#         #     random_state=random_state,\n",
    "#         # )\n",
    "#         self.outliers_vector = clf.fit_predict(np.column_stack((X, y))) == -1\n",
    "#         print(\"Outliers removed: {}\".format(self.outliers_vector.sum()))\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X, y=None):\n",
    "\n",
    "#         X = X[~self.outliers_vector]\n",
    "#         # y = y[~self.outliers_vector]\n",
    "\n",
    "#         print(\"Outliers removed: {}\".format(self.outliers_vector.sum()))\n",
    "#         print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "#         return X  # , y\n",
    "\n",
    "#     def set_params(self):\n",
    "#         return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_remover_step = Step(\n",
    "    \"features_remover\",\n",
    "    FeaturesRemover(),\n",
    "    {\"corr_threshold\": 0.95},\n",
    ")\n",
    "std_step = Step(\n",
    "    \"std\",\n",
    "    preprocessing.StandardScaler(),\n",
    ")\n",
    "minmax_step = Step(\n",
    "    \"minmax\",\n",
    "    preprocessing.MinMaxScaler(),\n",
    ")\n",
    "l1_step = Step(\n",
    "    \"l1\",\n",
    "    preprocessing.Normalizer(norm=\"l1\"),\n",
    ")\n",
    "l2_step = Step(\n",
    "    \"l2\",\n",
    "    preprocessing.Normalizer(norm=\"l2\"),\n",
    ")\n",
    "lmax_step = Step(\n",
    "    \"lmax\",\n",
    "    preprocessing.Normalizer(norm=\"max\"),\n",
    ")\n",
    "pca_step = Step(\n",
    "    \"pca\",\n",
    "    PCA(random_state=RANDOM_STATE),\n",
    "    {\n",
    "        \"n_components\": [0.95],\n",
    "        # \"whiten\": [True, False],\n",
    "        # \"svd_solver\": \"full\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(combinations: list[Combination], estimator_tag: str, save_results=True):\n",
    "\n",
    "    # Iterate over *all* combinations\n",
    "    for index, combination in enumerate(combinations):\n",
    "        print(\"\\nCombination {}/{}  |  {}\".format(index + 1, len(combinations), combination.tag))\n",
    "\n",
    "        # Check if this combination is already tested\n",
    "        if save_results:\n",
    "            file_name = OUTPUT_FOLDER + estimator_tag + \"_output.csv\"\n",
    "            if combination_already_tested(file_name, combination):\n",
    "                print(\"  ==> Already done. Skipped.\")\n",
    "                continue\n",
    "\n",
    "        gs = GridSearchCV(\n",
    "            estimator=combination.pipeline,\n",
    "            param_grid=combination.parameters,\n",
    "            n_jobs=-1,\n",
    "            cv=3,\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        # Fit\n",
    "        gs.fit(X_train, np.ravel(y_train))\n",
    "        # Predict\n",
    "        y_pred = gs.predict(X_test)\n",
    "        # Test scores\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        print(\"  ==> R2: {}\\tMSE: {}\".format(r2, mse))\n",
    "\n",
    "        # Save results\n",
    "        if save_results:\n",
    "            results = combination.set_MSE(mse).set_R2(r2).as_df()\n",
    "            if os.path.isfile(file_name):\n",
    "                outputs = pd.read_csv(file_name)\n",
    "                if not outputs.empty:\n",
    "                    results = pd.concat([outputs, results], axis=0)\n",
    "            results.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination 1/10  |  random_forest\n",
      "  ==> Already done. Skipped.\n",
      "\n",
      "Combination 2/10  |  minmax + random_forest\n",
      "  ==> Already done. Skipped.\n",
      "\n",
      "Combination 3/10  |  minmax + pca + random_forest\n",
      "  ==> Already done. Skipped.\n",
      "\n",
      "Combination 4/10  |  features_remover + minmax + pca + random_forest\n",
      "  ==> Already done. Skipped.\n",
      "\n",
      "Combination 5/10  |  std + random_forest\n",
      "  ==> Already done. Skipped.\n",
      "\n",
      "Combination 6/10  |  std + pca + random_forest\n",
      "  ==> Already done. Skipped.\n",
      "\n",
      "Combination 7/10  |  features_remover + std + pca + random_forest\n",
      "  ==> Already done. Skipped.\n",
      "\n",
      "Combination 8/10  |  minmax + l1 + random_forest\n",
      "  ==> Already done. Skipped.\n",
      "\n",
      "Combination 9/10  |  features_remover + minmax + l1 + random_forest\n",
      "  ==> Already done. Skipped.\n",
      "\n",
      "Combination 10/10  |  lmax + random_forest\n",
      "  ==> Already done. Skipped.\n"
     ]
    }
   ],
   "source": [
    "estimator_tag = \"random_forest\"\n",
    "random_forest_step = Step(\n",
    "    estimator_tag,\n",
    "    RandomForestRegressor(n_jobs=-1, random_state=RANDOM_STATE),\n",
    "    {\n",
    "        \"max_samples\": [0.66],\n",
    "        \"criterion\": [\"squared_error\"],\n",
    "        \"n_estimators\": [200],\n",
    "        \"max_depth\": [25],  # , 5, 10, 15, 20],\n",
    "        # \"max_leaf_nodes\": [None],  # 50, 100, 200, 300, 400\n",
    "        # \"ccp_alpha\": [0.0, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0],\n",
    "        \"max_features\": [0.4],  # \"sqrt\", \"log2\", 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.9\n",
    "    },\n",
    ")\n",
    "\n",
    "combinations = extract_combinations(\n",
    "    Pipe(random_forest_step),\n",
    "    Pipe(minmax_step, random_forest_step),\n",
    "    Pipe(minmax_step, pca_step, random_forest_step),\n",
    "    Pipe(features_remover_step, minmax_step, pca_step, random_forest_step),\n",
    "    Pipe(std_step, random_forest_step),\n",
    "    Pipe(std_step, pca_step, random_forest_step),\n",
    "    Pipe(features_remover_step, std_step, pca_step, random_forest_step),\n",
    "    Pipe(minmax_step, l1_step, random_forest_step),\n",
    "    Pipe(features_remover_step, minmax_step, l1_step, random_forest_step),\n",
    "    Pipe(lmax_step, random_forest_step),\n",
    ")\n",
    "grid_search(combinations, estimator_tag=estimator_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_ab836\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_ab836_level0_col0\" class=\"col_heading level0 col0\" >tag</th>\n",
       "      <th id=\"T_ab836_level0_col1\" class=\"col_heading level0 col1\" >R2</th>\n",
       "      <th id=\"T_ab836_level0_col2\" class=\"col_heading level0 col2\" >MSE</th>\n",
       "      <th id=\"T_ab836_level0_col3\" class=\"col_heading level0 col3\" >random_forest__max_samples</th>\n",
       "      <th id=\"T_ab836_level0_col4\" class=\"col_heading level0 col4\" >random_forest__criterion</th>\n",
       "      <th id=\"T_ab836_level0_col5\" class=\"col_heading level0 col5\" >random_forest__n_estimators</th>\n",
       "      <th id=\"T_ab836_level0_col6\" class=\"col_heading level0 col6\" >random_forest__max_depth</th>\n",
       "      <th id=\"T_ab836_level0_col7\" class=\"col_heading level0 col7\" >random_forest__max_features</th>\n",
       "      <th id=\"T_ab836_level0_col8\" class=\"col_heading level0 col8\" >pca__n_components</th>\n",
       "      <th id=\"T_ab836_level0_col9\" class=\"col_heading level0 col9\" >features_remover__corr_threshold</th>\n",
       "      <th id=\"T_ab836_level0_col10\" class=\"col_heading level0 col10\" >random_forest__max_leaf_nodes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_ab836_level0_row0\" class=\"row_heading level0 row0\" >4</th>\n",
       "      <td id=\"T_ab836_row0_col0\" class=\"data row0 col0\" >std + random_forest</td>\n",
       "      <td id=\"T_ab836_row0_col1\" class=\"data row0 col1\" >0.9269</td>\n",
       "      <td id=\"T_ab836_row0_col2\" class=\"data row0 col2\" >83.2301</td>\n",
       "      <td id=\"T_ab836_row0_col3\" class=\"data row0 col3\" >0.6600</td>\n",
       "      <td id=\"T_ab836_row0_col4\" class=\"data row0 col4\" >squared_error</td>\n",
       "      <td id=\"T_ab836_row0_col5\" class=\"data row0 col5\" >200</td>\n",
       "      <td id=\"T_ab836_row0_col6\" class=\"data row0 col6\" >25</td>\n",
       "      <td id=\"T_ab836_row0_col7\" class=\"data row0 col7\" >0.4000</td>\n",
       "      <td id=\"T_ab836_row0_col8\" class=\"data row0 col8\" >nan</td>\n",
       "      <td id=\"T_ab836_row0_col9\" class=\"data row0 col9\" >nan</td>\n",
       "      <td id=\"T_ab836_row0_col10\" class=\"data row0 col10\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ab836_level0_row1\" class=\"row_heading level0 row1\" >14</th>\n",
       "      <td id=\"T_ab836_row1_col0\" class=\"data row1 col0\" >std + random_forest</td>\n",
       "      <td id=\"T_ab836_row1_col1\" class=\"data row1 col1\" >0.9269</td>\n",
       "      <td id=\"T_ab836_row1_col2\" class=\"data row1 col2\" >83.2301</td>\n",
       "      <td id=\"T_ab836_row1_col3\" class=\"data row1 col3\" >0.6600</td>\n",
       "      <td id=\"T_ab836_row1_col4\" class=\"data row1 col4\" >squared_error</td>\n",
       "      <td id=\"T_ab836_row1_col5\" class=\"data row1 col5\" >200</td>\n",
       "      <td id=\"T_ab836_row1_col6\" class=\"data row1 col6\" >25</td>\n",
       "      <td id=\"T_ab836_row1_col7\" class=\"data row1 col7\" >0.4000</td>\n",
       "      <td id=\"T_ab836_row1_col8\" class=\"data row1 col8\" >nan</td>\n",
       "      <td id=\"T_ab836_row1_col9\" class=\"data row1 col9\" >nan</td>\n",
       "      <td id=\"T_ab836_row1_col10\" class=\"data row1 col10\" >'None'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ab836_level0_row2\" class=\"row_heading level0 row2\" >11</th>\n",
       "      <td id=\"T_ab836_row2_col0\" class=\"data row2 col0\" >minmax + random_forest</td>\n",
       "      <td id=\"T_ab836_row2_col1\" class=\"data row2 col1\" >0.9268</td>\n",
       "      <td id=\"T_ab836_row2_col2\" class=\"data row2 col2\" >83.2731</td>\n",
       "      <td id=\"T_ab836_row2_col3\" class=\"data row2 col3\" >0.6600</td>\n",
       "      <td id=\"T_ab836_row2_col4\" class=\"data row2 col4\" >squared_error</td>\n",
       "      <td id=\"T_ab836_row2_col5\" class=\"data row2 col5\" >200</td>\n",
       "      <td id=\"T_ab836_row2_col6\" class=\"data row2 col6\" >25</td>\n",
       "      <td id=\"T_ab836_row2_col7\" class=\"data row2 col7\" >0.4000</td>\n",
       "      <td id=\"T_ab836_row2_col8\" class=\"data row2 col8\" >nan</td>\n",
       "      <td id=\"T_ab836_row2_col9\" class=\"data row2 col9\" >nan</td>\n",
       "      <td id=\"T_ab836_row2_col10\" class=\"data row2 col10\" >'None'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ab836_level0_row3\" class=\"row_heading level0 row3\" >1</th>\n",
       "      <td id=\"T_ab836_row3_col0\" class=\"data row3 col0\" >minmax + random_forest</td>\n",
       "      <td id=\"T_ab836_row3_col1\" class=\"data row3 col1\" >0.9268</td>\n",
       "      <td id=\"T_ab836_row3_col2\" class=\"data row3 col2\" >83.2731</td>\n",
       "      <td id=\"T_ab836_row3_col3\" class=\"data row3 col3\" >0.6600</td>\n",
       "      <td id=\"T_ab836_row3_col4\" class=\"data row3 col4\" >squared_error</td>\n",
       "      <td id=\"T_ab836_row3_col5\" class=\"data row3 col5\" >200</td>\n",
       "      <td id=\"T_ab836_row3_col6\" class=\"data row3 col6\" >25</td>\n",
       "      <td id=\"T_ab836_row3_col7\" class=\"data row3 col7\" >0.4000</td>\n",
       "      <td id=\"T_ab836_row3_col8\" class=\"data row3 col8\" >nan</td>\n",
       "      <td id=\"T_ab836_row3_col9\" class=\"data row3 col9\" >nan</td>\n",
       "      <td id=\"T_ab836_row3_col10\" class=\"data row3 col10\" >nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_ab836_level0_row4\" class=\"row_heading level0 row4\" >21</th>\n",
       "      <td id=\"T_ab836_row4_col0\" class=\"data row4 col0\" >lmax + random_forest</td>\n",
       "      <td id=\"T_ab836_row4_col1\" class=\"data row4 col1\" >0.9267</td>\n",
       "      <td id=\"T_ab836_row4_col2\" class=\"data row4 col2\" >83.3483</td>\n",
       "      <td id=\"T_ab836_row4_col3\" class=\"data row4 col3\" >0.6600</td>\n",
       "      <td id=\"T_ab836_row4_col4\" class=\"data row4 col4\" >squared_error</td>\n",
       "      <td id=\"T_ab836_row4_col5\" class=\"data row4 col5\" >200</td>\n",
       "      <td id=\"T_ab836_row4_col6\" class=\"data row4 col6\" >25</td>\n",
       "      <td id=\"T_ab836_row4_col7\" class=\"data row4 col7\" >0.4000</td>\n",
       "      <td id=\"T_ab836_row4_col8\" class=\"data row4 col8\" >nan</td>\n",
       "      <td id=\"T_ab836_row4_col9\" class=\"data row4 col9\" >nan</td>\n",
       "      <td id=\"T_ab836_row4_col10\" class=\"data row4 col10\" >nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fec06c110f0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_results(OUTPUT_FOLDER + estimator_tag + \"_output.csv\", 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
