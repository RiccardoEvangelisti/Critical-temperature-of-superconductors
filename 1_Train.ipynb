{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critical Temperature of Superconductors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import itertools\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from utils import Step, Pipe, MultiplePipes\n",
    "\n",
    "import os\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "DATA_FOLDER = \"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "\n",
    "# REMOVE_HIGH_CORR_FEATURES = True\n",
    "# CORR_THRESHOLD = 0.95\n",
    "\n",
    "\n",
    "OUTLIER_REMOVAL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of Properties+Formula df:  (17010, 169)\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat(\n",
    "    [\n",
    "        pd.read_csv(DATA_FOLDER + \"formula_train.csv\").drop(columns=[\"critical_temp\"]),\n",
    "        pd.read_csv(DATA_FOLDER + \"train.csv\"),\n",
    "    ],\n",
    "    axis=1,\n",
    ")\n",
    "print(\"Shapes of Properties+Formula df: \", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"material\" feature\n",
    "df = df.drop(columns=\"material\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13608, 167), (3402, 167), (13608, 1), (3402, 1))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state=RANDOM_STATE)\n",
    "\n",
    "X_train = train.drop(columns=[\"critical_temp\"])\n",
    "y_train = train[[\"critical_temp\"]]\n",
    "\n",
    "X_test = test.drop(columns=[\"critical_temp\"])\n",
    "y_test = test[[\"critical_temp\"]]\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Remove Highly correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if REMOVE_HIGH_CORR_FEATURES:\n",
    "#     corr_matrix = df.corr().abs()\n",
    "#     upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "#     cols_to_drop = [column for column in upper.columns if any(upper[column] >= CORR_THRESHOLD)]\n",
    "\n",
    "#     print(\"{} Cols Removed: {}\".format(len(cols_to_drop), cols_to_drop))\n",
    "#     X_train = X_train.drop(columns=cols_to_drop)\n",
    "#     X_test = X_test.drop(columns=cols_to_drop)\n",
    "#     print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeaturesRemover:\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        corr_matrix = df.corr().abs()\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        cols_to_drop = [column for column in upper.columns if any(upper[column] >= self.corr_threshold)]\n",
    "\n",
    "        # print(\"{} Cols Removed: {}\".format(len(cols_to_drop), cols_to_drop))\n",
    "        X = X.drop(columns=cols_to_drop)\n",
    "        return X\n",
    "\n",
    "    def set_params(self, corr_threshold):\n",
    "        self.corr_threshold = corr_threshold\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OUTLIER_REMOVAL:\n",
    "    columns = train.columns\n",
    "    outliers = pd.Series(index=train.index, dtype=bool)\n",
    "\n",
    "    clf = LocalOutlierFactor(n_jobs=-1)\n",
    "    # clf = IsolationForest(\n",
    "    #     max_samples=1.0,\n",
    "    #     contamination=0.001,\n",
    "    #     n_jobs=-1,\n",
    "    #     random_state=random_state,\n",
    "    # )\n",
    "    outliers = clf.fit_predict(train) == -1\n",
    "\n",
    "    print(\"Outliers removed: {}\".format(outliers.sum()))\n",
    "    train = train[~outliers]\n",
    "\n",
    "    X_train = train.drop(columns=[\"critical_temp\"])\n",
    "    y_train = train[[\"critical_temp\"]]\n",
    "    print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class OutliersRemover:\n",
    "#     def __init__(self) -> None:\n",
    "#         self.outliers_vector = None\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         self.outliers_vector = pd.Series(index=X.index, dtype=bool)\n",
    "\n",
    "#         clf = LocalOutlierFactor(n_jobs=-1)\n",
    "#         # clf = IsolationForest(\n",
    "#         #     max_samples=1.0,\n",
    "#         #     contamination=0.001,\n",
    "#         #     n_jobs=-1,\n",
    "#         #     random_state=random_state,\n",
    "#         # )\n",
    "#         self.outliers_vector = clf.fit_predict(np.column_stack((X, y))) == -1\n",
    "#         print(\"Outliers removed: {}\".format(self.outliers_vector.sum()))\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X, y=None):\n",
    "\n",
    "#         X = X[~self.outliers_vector]\n",
    "#         y = y[~self.outliers_vector]\n",
    "\n",
    "#         print(\"Outliers removed: {}\".format(self.outliers_vector.sum()))\n",
    "#         print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "#         return X, y\n",
    "\n",
    "#     def set_params(self):\n",
    "#         return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_remover_step = Step(\n",
    "    \"features_remover\",\n",
    "    FeaturesRemover(),\n",
    "    {\"corr_threshold\": 0.95},\n",
    ")\n",
    "std_step = Step(\n",
    "    \"std\",\n",
    "    preprocessing.StandardScaler(),\n",
    ")\n",
    "minmax_step = Step(\n",
    "    \"minmax\",\n",
    "    preprocessing.MinMaxScaler(),\n",
    ")\n",
    "l1_step = Step(\n",
    "    \"l1\",\n",
    "    preprocessing.Normalizer(norm=\"l1\"),\n",
    ")\n",
    "l2_step = Step(\n",
    "    \"l2\",\n",
    "    preprocessing.Normalizer(norm=\"l2\"),\n",
    ")\n",
    "lmax_step = Step(\n",
    "    \"lmax\",\n",
    "    preprocessing.Normalizer(norm=\"max\"),\n",
    ")\n",
    "pca_step = Step(\n",
    "    \"pca\",\n",
    "    PCA(random_state=RANDOM_STATE),\n",
    "    {\n",
    "        \"n_components\": [0.95],\n",
    "        # \"whiten\": [True, False],\n",
    "        # \"svd_solver\": \"full\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(combinations: MultiplePipes, estimator_tag: str, save_results=True):\n",
    "\n",
    "    # Iterate over *all* combinations\n",
    "    for index, (pipeline, parameters, tag) in enumerate(combinations.combinations):\n",
    "        print(\"Combination {}/{}. Steps: {}\".format(index + 1, len(combinations.combinations), tag))\n",
    "\n",
    "        if save_results:\n",
    "            file_name = \"outputs/\" + \"1_\" + estimator_tag + \"_output.csv\"\n",
    "            if os.path.isfile(file_name):\n",
    "                outputs = pd.read_csv(file_name).loc[:, parameters.keys()]\n",
    "                #TODO\n",
    "                if pd.DataFrame(parameters).isin(outputs).all().any():\n",
    "                    # if (outputs == parameters).all(1).any():\n",
    "                    print(\"\\tAlready done. Skipped.\")\n",
    "                    continue\n",
    "\n",
    "        gs = GridSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_grid=parameters,\n",
    "            scoring=\"r2\",\n",
    "            n_jobs=-1,\n",
    "            cv=3,\n",
    "            verbose=0,\n",
    "        )\n",
    "\n",
    "        # Fit\n",
    "        gs.fit(X_train, np.ravel(y_train))\n",
    "        # Predict\n",
    "        y_pred = gs.predict(X_test)\n",
    "        # Test scores\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        print(\"MSE: {}\\tR2: {}\".format(mse, r2))\n",
    "\n",
    "        if save_results:\n",
    "            outputs = pd.DataFrame(parameters or None, index=[0])\n",
    "            outputs = outputs.assign(tag=tag, MSE=mse, R2=r2)\n",
    "            outputs = pd.concat(\n",
    "                [pd.read_csv(file_name) if os.path.isfile(file_name) else pd.DataFrame(), outputs], axis=0\n",
    "            )\n",
    "            outputs.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination 1/6. Steps: minmax + pca + random_forest\n",
      "\tAlready done. Skipped.\n",
      "Combination 2/6. Steps: minmax + pca + random_forest\n",
      "\tAlready done. Skipped.\n",
      "Combination 3/6. Steps: minmax + pca + random_forest\n",
      "\tAlready done. Skipped.\n",
      "Combination 4/6. Steps: features_remover + minmax + pca + random_forest\n",
      "\tAlready done. Skipped.\n",
      "Combination 5/6. Steps: features_remover + minmax + pca + random_forest\n",
      "\tAlready done. Skipped.\n",
      "Combination 6/6. Steps: features_remover + minmax + pca + random_forest\n",
      "\tAlready done. Skipped.\n"
     ]
    }
   ],
   "source": [
    "estimator_tag = \"random_forest\"\n",
    "random_forest_step = Step(\n",
    "    estimator_tag,\n",
    "    RandomForestRegressor(n_jobs=-1, random_state=RANDOM_STATE),\n",
    "    {\n",
    "        \"max_samples\": [0.66, 0.33, 0.2],\n",
    "        \"criterion\": [\"squared_error\"],\n",
    "        \"n_estimators\": [200],\n",
    "        \"max_depth\": [25],  # , 5, 10, 15, 20],\n",
    "        \"max_leaf_nodes\": [None],  # 50, 100, 200, 300, 400\n",
    "        \"ccp_alpha\": [0.01],  # 0.0, 0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0\n",
    "        \"max_features\": [0.4],  # \"sqrt\", \"log2\", 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.9\n",
    "    },\n",
    ")\n",
    "\n",
    "combinations = MultiplePipes(\n",
    "    Pipe(minmax_step, pca_step, random_forest_step),\n",
    "    Pipe(features_remover_step, minmax_step, pca_step, random_forest_step),\n",
    ")\n",
    "grid_search(combinations, estimator_tag=estimator_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## Search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
